<!DOCTYPE html>
<html>
<head>
<meta charset='utf-8'>
<meta content='IE=edge;chrome=1' http-equiv='X-UA-Compatible'>
<meta content='width=device-width, initial-scale=1' name='viewport'>
<link href='/favicon/apple-touch-icon-57x57-8e6554ae.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/favicon/apple-touch-icon-114x114-e0cb1072.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href='/favicon/apple-touch-icon-72x72-a86b7103.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/favicon/apple-touch-icon-144x144-bd50bc10.png' rel='apple-touch-icon-precomposed' sizes='144x144'>
<link href='/favicon/apple-touch-icon-60x60-ca47953b.png' rel='apple-touch-icon-precomposed' sizes='60x60'>
<link href='/favicon/apple-touch-icon-120x120-068de821.png' rel='apple-touch-icon-precomposed' sizes='120x120'>
<link href='/favicon/apple-touch-icon-76x76-2bd34869.png' rel='apple-touch-icon-precomposed' sizes='76x76'>
<link href='/favicon/apple-touch-icon-152x152-da7652e0.png' rel='apple-touch-icon-precomposed' sizes='152x152'>
<link href='/favicon/favicon-196x196-569e78d9.png' rel='icon' sizes='196x196' type='image/png'>
<link href='/favicon/favicon-96x96-d809406c.png' rel='icon' sizes='96x96' type='image/png'>
<link href='/favicon/favicon-32x32-162b5c6d.png' rel='icon' sizes='32x32' type='image/png'>
<link href='/favicon/favicon-16x16-d0c2b051.png' rel='icon' sizes='16x16' type='image/png'>
<link href='/favicon/favicon-128-2f649ba2.png' rel='icon' sizes='128x128' type='image/png'>
<meta content='Â ' name='application-name'>
<meta content='#FFFFFF' name='msapplication-TileColor'>
<meta content='/favicon/mstile-144x144-bd50bc10.png' name='msapplication-TileImage'>
<meta content='/favicon/mstile-70x70-2f649ba2.png' name='msapplication-square70x70logo'>
<meta content='/favicon/mstile-150x150-999c62ba.png' name='msapplication-square150x150logo'>
<meta content='/favicon/mstile-310x150-37756fac.png' name='msapplication-wide310x150logo'>
<meta content='/favicon/mstile-310x310-6bd894f0.png' name='msapplication-square310x310logo'>
<title>
Migrating to Amazon Elastic Map Reduce (EMR)
</title>
<link href="../../../../../assets/stylesheets/all-3d0a9e1d.css" rel="stylesheet" />
<script src="../../../../../assets/javascripts/all-f0e3b20f.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-7406390-2', 'auto');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
  
    // outbound link tracking
    // http://www.sitepoint.com/track-outbound-links-google-analytics/
    (function($) {
      "use strict";
      // current page host
      var baseURI = window.location.host;
      // click event on body
      $("body").on("click", function(e) {
        // abandon if link already aborted or analytics is not available
        if (e.isDefaultPrevented() || typeof ga !== "function") return;
        // abandon if no active link or link within domain
        var link = $(e.target).closest("a");
        if (link.length != 1 || baseURI == link[0].host) return;
        // cancel event and record outbound link
        e.preventDefault();
        var href = link[0].href;
        ga('send', {
          'hitType': 'event',
          'eventCategory': 'outbound',
          'eventAction': 'link',
          'eventLabel': href,
          'hitCallback': loadPage
        });
        // redirect after one second if recording takes too long
        setTimeout(loadPage, 1000);
        // redirect to outbound page
        function loadPage() {
          document.location = href;
        }
      });
    })(jQuery);
</script>

<link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/blog/feed.xml" />
</head>
<body>
<nav class='navbar navbar-default' id='top' role='navigation'>
<div class='container'>
<div class='navbar-header'>
<div class='navbar-brand'>
<a href='/blog/'>
<img src="../../../../../assets/images/home/smile-0a95f89f.jpg" />
</a>
<span>Migrating to Amazon Elastic Map Reduce (EMR)</span>
</div>
<button class='navbar-toggle' data-target='.navbar-collapse' data-toggle='collapse' type='button'>
<span class='icon-bar'></span>
<span class='icon-bar'></span>
<span class='icon-bar'></span>
</button>
<div class='navbar-right collapse navbar-collapse' id='bs-nav-collapse'>
<ul class='nav navbar-nav'>
<li>
<form class='search navbar-form' role='search'>
<div class='form-group'>
<input class='form-control bloodhound typeahead' data-articles='[["Eachine VR-007 Goggle Review","/blog/2016/04/25/eachine-vr-007-goggle-review/"],["AirBot Flip32 F3 AIO Lite and Typhoon 4-in-1 ESC","/blog/2016/04/11/airbot-flip32-f3-aio-lite-and-typhoon-4-in-1-esc/"],["Turnigy Reaktor 250W Charger","/blog/2016/04/02/turnigy-reaktor-250w-charger/"],["HobbyKing Graphene and Bolt High Voltage LiPo Battery Analysis","/blog/2016/03/25/hobbyking-graphene-battery-analysis/"],["BetaFlight in-flight PID tuning","/blog/2016/03/24/betaflight-in-flight-pid-tuning/"],["Drone Building at Maker Faire Bay Area 2016","/blog/2016/03/23/drone-building-at-maker-faire-bay-area-2016/"],["AVR Dissassembly","/blog/2016/02/25/avr-dissassembly/"],["Victory230 FPV Mini-quad Setup and Review","/blog/2016/02/10/rctimer-victory230-fpv-miniquad/"],["FrSky RX SBUS and Frsky DJT Module","/blog/2016/02/09/airwolf-diy-frsky-rx-sbus/"],["HMDVR FPV Video Recorder","/blog/2016/02/06/hmdvr-fpv-video-recorder/"],["Rctimer BeerotorF3 Flight Controller","/blog/2016/02/03/rctimer-beerotorf3-flight-controller/"],["Flying Sony NEX Micro 4/3rds Camera","/blog/2016/01/29/flying-micro-4-3rds-camera/"],["Q600 Quadcopter","/blog/2016/01/29/rctimer-q600-quadcopter/"],["ASP micro 4/3rds Gimbal and SimpleBGC 32-bit Gimbal Controller","/blog/2016/01/29/rctimer-asp-micro-4-3rds-gimbal-and-simplebgc-32-bit-gimbal-controller/"],["PixHawk Flashing and Setup","/blog/2016/01/29/rctimer-fixhawk-pixhawk-flashing-and-setup/"],["BlHeli ESCs","/blog/2015/12/06/blheli-esc-identification/"],["BlHeli ESC Upgrading and Configuration","/blog/2015/12/06/blheli-configuration/"],["BlHeli CleanFlight ESC Calibration","/blog/2015/12/06/blheli-calibration/"],["Rctimer Micro 160 BeeRotor Build, Setup and Review","/blog/2015/12/05/rctimer-micro-160-beerotor-build-setup-and-review/"],["Flashing SiLabs xRotor 20a with BlHeli via C2 4w-if","/blog/2015/11/16/flashing-xrotor-20a-with-blheli-via-4w-if/"],["Rctimer OZE32 / RTFQ Flip32 AIO Integrated Flight Controller Review and Setup","/blog/2015/11/09/rctimer-oze32-integrated-flight-controller-review-and-setup/"],["Rooting Marshmellow, keeping SELinux","/blog/2015/11/03/rooting-marshmellow-keeping-selinux/"],["MultiRotor Setup Checklist","/blog/2015/10/29/multirotor-setup-checklist/"],["ST-LINK flash and debug on the SPRacingF3","/blog/2015/10/24/st-link-flash-and-debug-on-the-spracingf3-discovery/"],["Debugging Android Native (NDK) Apps","/blog/2015/09/27/debugging-android-native-ndk-apps/"],["Fixing an Arduino 0x000000 Error","/blog/2015/09/16/fixing-an-arduino-0x000000-error/"],["GPS VK16U6 CleanFlight Setup","/blog/2015/08/26/gps-vk16u6-cleanflight-setup/"],["Go FPV Latency Testing","/blog/2015/08/24/go-fpv-latency-testing/"],["Building Android on Ubuntu 14.04","/blog/2015/07/23/building-android-on-ubuntu-14-04/"],["Migrating to Amazon Elastic Map Reduce (EMR)","/blog/2015/07/15/migrating-to-amazon-elastic-map-reduce-emr/"],["Naze32 ST-Link Debugging","/blog/2015/07/06/naze32-st-link-debugging/"],["Decompiling an APK","/blog/2015/06/30/decompiling-an-apk/"],["Nexpose Community Edition copy Scan Template","/blog/2015/06/17/nexpose-community-edition-copy-scan-template/"],["Wireless Video Tx/Rx FPV Channels","/blog/2015/06/17/boscam-tx-rx-fpv-channels/"],["Making a UHF 433mhz Inverted Vee Antenna Set","/blog/2015/06/15/making-a-uhf-433mhz-inverted-vee-antenna-set/"],["Setup OrangeLRS with OpenLRSng","/blog/2015/05/21/setup-orangelrs-with-openlrsng/"],["Go FPV Goggles","/blog/2015/05/21/go-fpv-goggles/"],["FPV250 with Cleanflight, Minimosd, OpenLRSng and a Xiaomi Yi GoPro Clone","/blog/2015/05/19/fpv250-cleanflight/"],["Flashing BlHeli on HobbyKing BlueSeries 12a ESCs With a Chinese Arduino Nano Clone","/blog/2015/05/15/flashing-blheli-on-hobbyking-blueseries-12a-escs-with-a-chinese-arduino-nano-clone/"],["Flashing NodeMCU on the ESP8266","/blog/2015/05/14/flashing-nodemcu-on-the-esp8266/"],["Ebay HC-05 Bluetooth Setup Guide","/blog/2015/05/12/ebay-hc-05-bluetooth-setup-guide/"],["3D Camera on the Tricopter","/blog/2015/05/12/3d-camera-on-the-tricopter/"],["ProRes editing in Premier","/blog/2015/05/11/prores-editing-in-photoshop/"],["Xiaomi Yi -- The Ant Camera","/blog/2015/05/08/xiaomi-yi-the-ant-camera/"],["Minim OSD","/blog/2015/04/22/minim-osd/"],["Flashing BlHeli on HobbyKing F20 ESCs With a Chinese Arduino Nano Clone","/blog/2015/04/20/flashing-blheli-on-hobbyking-f20-escs-with-a-chinese-arduino-nano-clone/"],["Handling Dates in Hadoop Pig","/blog/2015/04/17/handling-dates-in-hadoop-pig/"],["LiPo Battery Price Analysis","/blog/2015/04/10/lipo-battery-price-analysis/"],["ADB over WiFi (aka, how do I debug when using USB host mode?)","/blog/2015/03/27/adb-over-wifi-aka-how-do-i-debug-when-using-usb-host-mode/"],["Shrink a linux VM on OSX","/blog/2015/03/18/shrink-a-linux-vm-on-osx/"],["Mounting a VHD NTFS image on Ubuntu","/blog/2015/02/27/mounting-a-vhd-ntfs-image-on-ubuntu/"],["Ubuntu on the Beaglebone Black","/blog/2015/01/16/ubuntu-on-the-beaglebone-black/"],["Configuring the HK-T6A for a Flight Simulator","/blog/2015/01/02/configuring-the-hk-t6a-for-a-flight-simulator/"],["A complete noob&#x0027;s guide to RC","/blog/2015/01/02/a-complete-noobs-guide-to-rc/"],["DropCam on a Captive Portal","/blog/2014/11/17/dropcam-on-a-captive-portal/"],["Java reversing tips","/blog/2014/09/18/java-reversing-tips/"],["Ooize Data Pipeline Done-Flag","/blog/2014/09/02/oozie-data-pipeline-done-flag/"],["Time Machine on exFat","/blog/2014/08/06/Time_Machine_on_exFat/"],["Optimizing Postgresql","/blog/2014/01/31/Optimizing_Postgresql/"],["Mail Maker","/blog/2014/01/24/Mail_Maker/"],["Angularjs and Rails","/blog/2013/07/12/Angularjs_and_Rails/"],["nginx ssl chained certs","/blog/2013/04/24/nginx_ssl_chained_certs/"],["rails log tagging","/blog/2013/04/24/rails_log_tagging/"],["Converting an Android Project to Maven","/blog/2013/04/16/Converting_an_Android_Project_to_Maven/"],["Oh! Map my rails!","/blog/2013/04/09/Oh_Map_my_rails/"],["mysql permissions","/blog/2013/04/09/mysql_permissions/"],["reset mysql slave","/blog/2013/03/29/reset_mysql_slave/"],["installing rmagick on 10.8 with homebrew","/blog/2013/02/14/installing_rmagick_on_108_with_homebrew/"],["NP-Completeness","/blog/2013/02/08/NP-Completeness/"],["Find and replace non-breaking spaces in vim","/blog/2013/02/01/Find_and_replace_non-breaking_spaces_in_vim/"],["Open Resty the Prebuilt Reverse Proxy","/blog/2013/02/01/Open_Resty_the_Prebuilt_Reverse_Proxy/"],["Sublime Text 2 Tweaks","/blog/2012/12/17/Sublime_Text_2_Tweaks/"],["Batch Convert CR2 to JPG","/blog/2012/12/01/Batch_Convert_CR2_to_JPG/"],["Androids in the wild","/blog/2012/06/30/Androids_in_the_wild/"],["Why you need database_cleaner","/blog/2012/06/11/Why_you_need_database_cleaner/"],["Find and replace in files on linux/unix","/blog/2012/05/12/Find_and_replace_in_files_on_linuxunix/"],["a pithy guide to railsconf 2012","/blog/2012/04/26/a_pithy_guide_to_railsconf_2012/"],["faster rails boot ruby 1.9.3 + debugger","/blog/2012/04/12/faster_rails_boot_ruby_193_debugger/"],["homebrew -- fix that nasty mysql.server start segfault","/blog/2012/03/29/homebrew_--_fix_that_nasty_mysqlserver_start_segfault/"],["read/write filesystem on rooted android for an ad-free android experience","/blog/2012/02/14/readwrite_filesystem_on_rooted_android_for_an_ad-free_android_experience/"],["Turn Wikipedia Back On","/blog/2012/01/17/Turn_Wikipedia_Back_On/"],["compiling qt4 on centos 5","/blog/2012/01/06/compiling_qt4_on_centos_5/"],["pip install pymongo with xcode 4","/blog/2011/08/09/pip_install_pymongo_with_xcode_4/"],["interactive python history -- better titled \"gimme some up arrow love\"","/blog/2011/08/05/interactive_python_history_--_better_titled_gimme_some_up_arrow_love/"],["Centos 5.5 yum repo sha256 fix","/blog/2011/07/27/Centos_55_yum_repo_sha256_fix/"],["Apple Keyboard for Android!","/blog/2011/07/15/Apple_Keyboard_for_Android/"],["jQuery Everywhere","/blog/2011/05/24/jQuery_Everywhere/"],["Django Migrations","/blog/2011/05/11/Django_Migrations/"],["RVM + Rails + Git + HTML5 Boiler Plate... What more could you possibly want?","/blog/2011/02/22/RVM_Rails_Git_HTML5_Boiler_Plate_What_more_could_you_possibly_want/"],["OneCast 2.0 - downloadable podcasts for androids of every color","/blog/2011/02/18/OneCast_20_-_downloadable_podcasts_for_androids_of_every_color/"],["Default Android Menu Icons in Your App","/blog/2011/02/17/Default_Android_Menu_Icons_in_Your_App/"],["Windows 7 x64 Android Development (ADB)","/blog/2010/11/06/Windows_7_x64_Android_Development_ADB/"],["Firefox Plugin - Watching for an Application Exit Event","/blog/2010/05/03/Firefox_Plugin_-_Watching_for_an_Application_Exit_Event/"],["Stop Redrawing a Fixed Orientation Android App","/blog/2010/03/08/Stop_Redrawing_a_Fixed_Orientation_Android_App/"]]' placeholder='Search' type='text'>
</div>
</form>
</li>
</ul>
</div>
</div>
</div>
</nav>

<div class='container'>
<div class='row row-offcanvas row-offcanvas-left'>
<!-- sidebar -->
<div class='col-xs-3 col-sm-3 sidebar-offcanvas'>
<div id='sidebar' role='navigation'>
<div class='sidebar-contents'>
<div class='row'>
<div class='col-lg-12 page-header'>
<a href='#top'>Contents</a>
</div>
</div>
<div class='row'>
<div class='col-lg-12 table-of-contents'>
<ul class="nav"><li><a class="type-h3" href="#starting-a-cluster"><h3>Starting a cluster</h3></a></li>
<li><a class="type-h3" href="#configuration"><h3>Configuration</h3></a></li><ul><li><a class="type-h4" href="#hcatalog"><h4>HCatalog</h4></a></li>
<li><a class="type-h4" href="#sqoop"><h4>Sqoop</h4></a></li></ul>
<li><a class="type-h3" href="#running-hive"><h3>Running Hive</h3></a></li>
<li><a class="type-h3" href="#running-pig-with-usehcatalog"><h3>Running Pig with -useHCatalog</h3></a></li>
<li><a class="type-h3" href="#transforming-data-with-hive"><h3>Transforming data with hive</h3></a></li>
<li><a class="type-h3" href="#running-sqoop"><h3>Running Sqoop</h3></a></li>
<li><a class="type-h3" href="#handling-data-swapping-in-your-production-db"><h3>Handling data swapping in your production db</h3></a></li>
<li><a class="type-h3" href="#hive-elasticsearch-integration"><h3>Hive Elasticsearch integration</h3></a></li>
<li><a class="type-h3" href="#managing-underlying-tasks"><h3>Managing Underlying Tasks</h3></a></li>
<li><a class="type-h3" href="#troubleshooting"><h3>Troubleshooting</h3></a></li>
<li><a class="type-h3" href="#security-groups"><h3>Security groups</h3></a></li>
<li><a class="type-h3" href="#emr-4-1-changes"><h3>EMR 4.1 Changes</h3></a></li></ul>
</div>
</div>
<div class='row'>
<div class='col-lg-12 page-header'>
Related Articles
</div>
</div>
<div class='row'>
<div class='col-lg-12 related'>
<ul>
<li><a href="/blog/2015/04/20/flashing-blheli-on-hobbyking-f20-escs-with-a-chinese-arduino-nano-clone/">Flashing BlHeli on HobbyKing F20 ESCs With a Chinese Arduino Nano Clone</a></li>
<li><a href="/blog/2016/04/11/airbot-flip32-f3-aio-lite-and-typhoon-4-in-1-esc/">AirBot Flip32 F3 AIO Lite and Typhoon 4-in-1 ESC</a></li>
<li><a href="/blog/2016/03/25/hobbyking-graphene-battery-analysis/">HobbyKing Graphene and Bolt High Voltage LiPo Battery Analysis</a></li>
<li><a href="/blog/2015/12/06/blheli-configuration/">BlHeli ESC Upgrading and Configuration</a></li>
<li><a href="/blog/2016/02/09/airwolf-diy-frsky-rx-sbus/">FrSky RX SBUS and Frsky DJT Module</a></li>
<li><a href="/blog/2015/05/21/go-fpv-goggles/">Go FPV Goggles</a></li>
</ul>
</div>
</div>
<div class='row'>
<div class='col-lg-12 page-header'></div>
</div>
<div class='row'>
<div class='col-lg-12 text-center' style='padding:1em;'>
<a href='/blog/2016/03/23/drone-building-at-maker-faire-bay-area-2016'><img style="width: 50%" src="../../../../../assets/images/blog/MF16BA_Badge_255-83ebf429.png" /></a>
<div class='build'>
Build a Drone with me!
</div>
</div>
</div>
<div class='row'>
<div class='col-lg-12 text-center'>
Checkout my other guides at
<a href='/blog'>nathan.vertile.com/blog</a>
</div>
</div>
<div class='row'>
<div class='col-lg-12 page-header text-center'>
<div class='drawer-footer'>
<small>
Â©
<a href='mailto:nathan@vertile.com'>Nathan Tsoi</a>
</small>
</div>
</div>
</div>
<div class='row last'>
<div class='col-lg-12 text-center'>
<a class='btn-sm btn-default' href='https://github.com/nathantsoi' target='_blank'>
<i class='fa fa-github'></i>
</a>
<a class='btn-sm btn-default' href='https://bitbucket.org/nathantsoi' target='_blank'>
<i class='fa fa-bitbucket'></i>
</a>
<a class='btn-sm btn-default' href='https://twitter.com/nathantsoi/' target='_blank'>
<i class='fa fa-twitter'></i>
</a>
<a class='btn-sm btn-default' href='https://angel.co/nathantsoi' target='_blank'>
<i class='fa fa-angellist'></i>
</a>
<a class='btn-sm btn-default' href='https://linkedin.com/in/nattsoi/' target='_blank'>
<i class='fa fa-linkedin'></i>
</a>
<a class='btn-sm btn-default' href='https://facebook.com/nathantsoi' target='_blank'>
<i class='fa fa-facebook'></i>
</a>
<a class='btn-sm btn-default' href='https://plus.google.com/112518806102274696668' target='_blank'>
<i class='fa fa-google-plus'></i>
</a>
<a class='btn-sm btn-default' href='https://news.ycombinator.com/user?id=natvert' target='_blank'>
<i class='fa fa-hacker-news'></i>
</a>
<a class='btn-sm btn-default' href='https://trello.com/nathantsoi' target='_blank'>
<i class='fa fa-trello'></i>
</a>
</div>
</div>
</div>
</div>
</div>

<!-- main area -->
<div class='col-sm-9 col-xs-12' id='mainbar'>
<article>
<p>I recently migrated some of our data pipelines from our local Ambari manged cluster to Amazon Elastic Map Reduce to take advantage of the great cluster startup times, allowing scalable bootstrapping of clusters as necessary (and their subsequent termination).</p>

<p>The process was actually more difficult than I anticipated. <a href="http://aws.amazon.com/elasticmapreduce/">The overview page</a> describes lots of magical tools including hive and sqoop, but when it comes to implementing them, you&#39;re basically on your own... well not really, on your own with Amazon support (which is awesome). At times, the docs are non-existant.</p>

<p>Below, I&#39;ve tried to document some of the magic I learned as I went through the migration process. Hope it helps you make your data pipelines faster and more resource efficent.</p>

<h3 id="starting-a-cluster">Starting a cluster</h3>

<p>I wrapped my Amazon EMR app in Ruby and Rake, so I can easily configure the cluster options, but at the core is this command. This is what I use to launch the cluster, specifying the bootstrap actions and steps.</p>

<p><code>config/emr.rb</code>:</p>
<pre class="highlight plaintext"><code>module Config
  class EMR
    def initialize opts={}
      @root_path = File.expand_path(File.join(File.dirname(__FILE__), '..'))
      @shared_path = File.join(@root_path, 'config/aws')
      @bucket = opts[:bucket]||'example-etl-app'
      raise 'Specify an :app name in opts' if (@app = opts[:app]).nil?
    end

    def cfg file, opts={}
      if opts[:shared].nil?
        File.join(@root_path, "#{@app}-workflow", 'aws', file)
      else
        File.join(@shared_path, file)
      end
    end

    def bucket
      @bucket
    end

    def create_cluster
      bootstrap_actions =
        if File.exists?(bootstrap_file = cfg('bootstrap.json'))
          "--bootstrap-actions file://#{bootstrap_file}"
        else
          ""
        end
      # http://docs.aws.amazon.com/cli/latest/reference/emr/create-cluster.html
      cmd = &lt;&lt;-EOS
        aws emr --debug \
        create-cluster --ami-version=3.3.0 \
        --log-uri s3://#{bucket}/log \
        --enable-debugging \
        --ec2-attributes file://#{cfg('ec2_attributes_emr.json', shared: true)} \
        --applications Name=Hue Name=Hive Name=Pig \
        --use-default-roles \
        --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.xlarge InstanceGroupType=CORE,InstanceCount=2,InstanceType=m1.large \
        --steps file://#{cfg('steps.json')} #{bootstrap_actions}
      EOS
      puts cmd
      system cmd
    end

    def self.run_cluster app
      emr = Config::EMR.new app: app
      emr.create_cluster
    end

  end
end
</code></pre>

<p>Documentation for the AWS CLI EMR is here: <a href="http://docs.aws.amazon.com/cli/latest/reference/emr/index.html">http://docs.aws.amazon.com/cli/latest/reference/emr/index.html</a></p>

<p>A workflow is a series of steps and can now be run with rake:</p>

<p><code>emr.rake</code>:</p>
<pre class="highlight plaintext"><code>require './config/emr.rb'
desc "run the cluster"
task :emr do |t, args|
  %w/ an_etl another_etl /.each do |app|
    Config::EMR.run_cluster app
  end
end

</code></pre>

<p>In my project folder, each pipeline is organized into folders named:</p>
<pre class="highlight plaintext"><code>my-cool-app/
  first-pipeline-workflow/
    aws/
      steps.json
      bootstrap.json
    hive/
      transform-some-data.hive.sql
  another-pipeline-workflow/
    aws/
      steps.json
      bootstrap.json
    pig/
      load-some-data.pig
  ...
</code></pre>

<p>Shared script or organized by the tool name:</p>
<pre class="highlight plaintext"><code>my-cool-app/
  hive/
    create-schema.hive.sql
  python/
    mysql-to-hive-schema-translator.py
  ...
</code></pre>

<h3 id="configuration">Configuration</h3>

<p>I like to put all the configuration tasks before any ETL tasks so we can fail the cluster as early as possible if there is a setup problem.</p>

<p>Currently, I place each setup step in <code>steps.json</code> for pipeline it is needed in. Since there is duplicated code, these should be moved into a common location and compiled by rake into the <code>steps.json</code> file. I&#39;ll probably move these configs to a yaml file that can be translated into json.</p>

<p>You&#39;ll notice I&#39;ve set: <code>&quot;ActionOnFailure&quot;: &quot;CANCEL_AND_WAIT&quot;</code>, this is for development. In production, these should be switched to <code>TERMINATE_CLUSTER</code> to avoid any inactive long running clusters.</p>

<h4 id="hcatalog">HCatalog</h4>

<p>I use HCatalog in pig to load csvs into Hive</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Configure HCatalog"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://support.elasticmapreduce/bootstrap-actions/hive/0.13.0/hcatalog_configurer.rb"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<h4 id="sqoop">Sqoop</h4>

<p>Used to import/export data from my application layer db -- postgres.</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Install Sqoop"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://support.elasticmapreduce/bootstrap-actions/sqoop/install-sqoop-v2"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Then copy in any postgres connector you might need:</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Install Sqoop Postgres"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/aws/install_sqoop_postgres.sh"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Where <code>install_sqoop_postgres.sh</code>:</p>
<pre class="highlight shell"><code><span class="c">#!/bin/bash</span>
<span class="c"># the postgres jar is on the machine, just not in the right place, yet</span>
cp /usr/lib/oozie/libtools/postgresql-9.0-801.jdbc4.jar /home/hadoop/sqoop/lib/
</code></pre>

<h3 id="running-hive">Running Hive</h3>

<p>Start with the hive step runner:</p>
<pre class="highlight json"><code><span class="w">  </span><span class="p">{</span><span class="w">
    </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Hive Transform Companies"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"-f"</span><span class="p">,</span><span class="w"> </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/hive/transform_companies.hive.sql"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"HIVE"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre>

<p>If you get an error about specifying base path <code>ERROR missing required argument base-path</code>, use this instead:</p>
<pre class="highlight json"><code><span class="w">  </span><span class="p">{</span><span class="w">
    </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Hive Setup Databases and UDFs"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"s3://elasticmapreduce/libs/hive/hive-script"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"--base-path"</span><span class="p">,</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/hive/"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"--run-hive-script"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--hive-versions"</span><span class="p">,</span><span class="w"> </span><span class="s2">"0.13.1"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"--args"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"-f"</span><span class="p">,</span><span class="w"> </span><span class="s2">"s3://my-cool-app/current/hive/setup.hive.sql"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CONTINUE"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Where <code>setup.hive.sql</code> is the hive script you want to run.</p>

<h3 id="running-pig-with-usehcatalog">Running Pig with -useHCatalog</h3>

<p>Load some data</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Pig Stage CSVs"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://support.elasticmapreduce/libs/pig/pig-script-run"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--run-pig-script"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"--args"</span><span class="p">,</span><span class="w"> </span><span class="s2">"-useHCatalog"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"-f"</span><span class="p">,</span><span class="w"> </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/pig/stage-csvs.pig"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Whate stage-csvs.pig is something like:</p>
<pre class="highlight plaintext"><code>-- runwith: pig -useHCatalog
--  use the -w flag to show detailed warnings

REGISTER 'file:/home/hadoop/lib/pig/piggybank.jar'
DEFINE CSVExcelStorage org.apache.pig.piggybank.storage.CSVExcelStorage(
  ',', 'YES_MULTILINE', 'NOCHANGE', 'SKIP_INPUT_HEADER'
);


-- get the csvs from this year and last year
daily_raw_csvs = LOAD 's3n://my-cool-app/data/csvs/*201[45]-[0-9][0-9]-[0-9][0-9].csv' using CSVExcelStorage() AS (rank:int, rank_variation:chararry, category:chararray, period:chararray);

-- remove any empty rows
filtered_daily_raw_csvs = FILTER daily_raw_csvs BY (rank IS NOT NULL) AND (period IS NOT NULL);

-- remove any duplicates
distinct_csvs = DISTINCT filtered_daily_raw_csvs;

-- extract the date fields and rank_variation, which is a percent in the input data
with_dates = FOREACH distinct_csvs GENERATE rank, ((double)REGEX_EXTRACT(rank_variation, '([-]{0,1}\\d+)%', 1)/100) AS rank_variation, category, ToDate(REGEX_EXTRACT(period, '.*(\\d{4}-\\d{2}-\\d{2})$', 1)) AS period;

-- store the transformed data into hive, in the `stage` schema
STORE with_dates INTO 'stage.csvs' USING org.apache.hive.hcatalog.pig.HCatStorer();
</code></pre>

<h3 id="transforming-data-with-hive">Transforming data with hive</h3>

<p>Use the hive step runner listed above: </p>
<pre class="highlight json"><code><span class="w">  </span><span class="p">{</span><span class="w">
    </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Hive Transform Companies"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"-f"</span><span class="p">,</span><span class="w"> </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/hive/transform_ranks.hive.sql"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
    </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"HIVE"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Say we have rank data for multiple categories coming in on multiple rows (fully denormalized) and we want to normalize the category data.</p>

<p><code>transform_ranks.hive.sql</code> would look something like this:</p>
<pre class="highlight plaintext"><code>DROP TABLE IF EXISTS transformed.categories;
CREATE TABLE transformed.categories AS
  SELECT
         reflect("java.util.UUID", "randomUUID") AS id,
         C.category AS name,
         CAST(unix_timestamp() AS TIMESTAMP) AS created_at,
         CAST(unix_timestamp() AS TIMESTAMP) AS updated_at
    FROM stage.csvs C
;
</code></pre>

<p>Then when we transform ranks, we can reference the id we generated in the step above.</p>

<h3 id="running-sqoop">Running Sqoop</h3>

<p>Use the script-runner.jar</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Export to Sqoop"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/sqoop/export.sqoop.sh"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p>Where <code>export.sqoop.sh</code>:</p>
<pre class="highlight plaintext"><code># amazon emr warehouse directory
WAREHOUSE_DIR=/user/hive/warehouse

# table we're exporting
TABLE=categories

/home/hadoop/sqoop/bin/sqoop export \
  --verbose \
  --hcatalog-home /home/hadoop/.versions/hive-0.13.1/hcatalog \
  --connect jdbc:postgresql://mydbserver.mydomain.com:5432/db_name \
  --username dbusername --password dbpassword \
  --export-dir ${WAREHOUSE_DIR}/transformed.db/${TABLE} \
  --table ${TABLE}_temp \
  --input-fields-terminated-by '\0001' --input-null-string '\\N' --input-null-non-string '\\N'
</code></pre>

<h3 id="handling-data-swapping-in-your-production-db">Handling data swapping in your production db</h3>

<p>Observant readers will notice I import the transformed data into tables suffixed by <code>_temp</code>.</p>

<p>Since my webapp uses the postgres database in production, I&#39;ll need to cutover to the new data in a single transaction.</p>

<p>I run these postgres scripts from a script runner step:</p>

<p><code>pg_run.sh</code>:</p>
<pre class="highlight shell"><code><span class="c">#!/bin/bash</span>

<span class="c"># PARAMS:</span>
<span class="c">#  the pgparams config file name for the target database (config/postgres/), without .conf</span>
<span class="c">#  path from project home of sql file to run</span>

<span class="nb">set</span> -o nounset  <span class="c"># exit if trying to use an uninitialized var</span>
<span class="nb">set</span> -o errexit  <span class="c"># exit if any program fails</span>
<span class="nb">set</span> -o pipefail <span class="c"># exit if any program in a pipeline fails, also</span>
<span class="nb">set</span> -x          <span class="c"># debug mode</span>

<span class="nv">PGPARAMS</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">PGPARAMS_STRING</span><span class="o">=</span><span class="k">$(</span> hadoop fs -cat s3://my-cool-app/current/config/postgres/<span class="k">${</span><span class="nv">PGPARAMS</span><span class="k">}</span>.conf <span class="k">)</span>
<span class="nv">PG_SQL_PATH</span><span class="o">=</span><span class="nv">$2</span>
<span class="nv">PGCMD</span><span class="o">=</span><span class="k">$(</span> hadoop fs -cat <span class="k">${</span><span class="nv">PG_SQL_PATH</span><span class="k">}</span> <span class="k">)</span>
env <span class="nv">$PGPARAMS_STRING</span> psql -c <span class="s2">"</span><span class="k">${</span><span class="nv">PGCMD</span><span class="k">}</span><span class="s2">"</span>
</code></pre>

<p>steps:</p>
<pre class="highlight json"><code><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Postgres Indexes"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/postgres/pg_run.sh"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"dbname"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/postgres/indexes.pg.sql"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="err">,</span><span class="w">
</span><span class="p">{</span><span class="w">
  </span><span class="nt">"Name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Postgres Transformations"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Jar"</span><span class="p">:</span><span class="w"> </span><span class="s2">"s3://elasticmapreduce/libs/script-runner/script-runner.jar"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Args"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/postgres/pg_run.sh"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"dbname"</span><span class="p">,</span><span class="w">
    </span><span class="s2">"s3://my-cool-app/current/an-etl-workflow/postgres/transform.pg.sql"</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nt">"ActionOnFailure"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CANCEL_AND_WAIT"</span><span class="p">,</span><span class="w">
  </span><span class="nt">"Type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CUSTOM_JAR"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre>

<p><code>indexes.pg.sql</code>:</p>
<pre class="highlight plaintext"><code>CREATE INDEX index_category_names ON categories (name)
</code></pre>

<p><code>transform.pg.sql</code>:</p>
<pre class="highlight plaintext"><code>BEGIN;
  -- ranks
  ALTER TABLE categories_temp ADD PRIMARY KEY (id);
   DROP TABLE categories;
  ALTER TABLE categories_temp RENAME TO categories;
COMMIT;
</code></pre>

<h3 id="hive-elasticsearch-integration">Hive Elasticsearch integration</h3>

<p>run hive with elasticsearch, in debug mode:</p>
<pre class="highlight plaintext"><code>hive -hiveconf hive.aux.jars.path=elasticsearch-hadoop-hive-2.1.1.jar --hiveconf hive.root.logger=DEBUG,console
</code></pre>

<p>then use this to create tables:</p>
<pre class="highlight plaintext"><code>DROP TABLE elasticsearch_wigets;
CREATE EXTERNAL TABLE elasticsearch_wigets(
  `id` string,
  `name` string
)
STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler'
TBLPROPERTIES('es.nodes' = 'elasticsearch.awesome.com:9200',
              'es.resource' = 'wigets')
;
</code></pre>

<p>and set the data:</p>
<pre class="highlight plaintext"><code>INSERT OVERWRITE TABLE elasticsearch_wigets
    SELECT id,
           name
      FROM wigets
;
</code></pre>

<h3 id="managing-underlying-tasks">Managing Underlying Tasks</h3>

<p>use yarn application list (your results will show your ip, I&#39;ve redacted ours):</p>
<pre class="highlight plaintext"><code>[hadoop@ip-0-0-0-0 var]$ yarn application -list
15/07/15 23:37:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:9022
Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):1
                Application-Id      Application-Name      Application-Type        User       Queue               State         Final-State         Progress                        Tracking-URL
application_1436973673154_0030  calc_some_ranks_temp.jar             MAPREDUCE      hadoop     default             RUNNING           UNDEFINED           73.91% http://ip-0-0-0-0.ourdomain.com:36429
</code></pre>

<h3 id="troubleshooting">Troubleshooting</h3>

<ul>
<li>getting underlying jobs</li>
</ul>
<pre class="highlight plaintext"><code>yarn application -list
yarn application -kill &lt;application-id&gt;
</code></pre>

<ul>
<li><p>logs are in <code>/mnt/var</code></p>

<ul>
<li>step logs are in <code>/mnt/var/log/hadoop/steps</code></li>
</ul></li>
</ul>

<p>while a step is running you can strema the logs in realtime with these command on the master node:</p>
<pre class="highlight plaintext"><code>tail -f /mnt/var/log/hadoop/steps/s-54K87AJP83RR/*
</code></pre>

<ul>
<li>task logs are available via the <code>hadoop job -logs</code> command on the master node</li>
</ul>

<p>for example, if you see something like this in your hadoop logs, where one task is starting another:</p>
<pre class="highlight plaintext"><code>http://10.11.1.29:9026/taskdetails.jsp?jobid=job_1441139008487_0013&amp;tipid=task_1441139008487_0013_r_000009
</code></pre>

<p>find the logs for this sub-task with:</p>
<pre class="highlight plaintext"><code>hadoop job -logs job_1441139008487_0013
</code></pre>

<h3 id="security-groups">Security groups</h3>

<p>8443 -- all external management connections from the AWS EMR service happen over this port</p>

<p>The EMR management server IP block is:</p>
<pre class="highlight plaintext"><code>205.251.233.160/28
205.251.233.176/29
205.251.233.32/28
205.251.233.48/29
205.251.234.32/28
54.240.230.176/29
54.240.230.184/29
54.240.230.240/29
</code></pre>

<h3 id="emr-4-1-changes">EMR 4.1 Changes</h3>

<ul>
<li><p>hadoop install is pushed via package installation infrastructure</p></li>
<li><p>bootstrap actions are no longer used, instead everything is a step</p></li>
</ul>

<p>Let me know if I copy+pasted something wrong or if you have any questions. Leave a comment!</p>

<div class='text-center' style='margin: 1em'>
Checkout my other guides at
<a href='/blog'>nathan.vertile.com/blog</a>
</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
//<![CDATA[
                  var disqus_shortname = 'vertile';
          
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
//]]>
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

</article>

</div>
</div>
</div>
<script>
  var vglnk = { key: 'ddedf6f00d454f08b923376f1cb1cfbf' };
  
  (function(d, t) {
    var s = d.createElement(t); s.type = 'text/javascript'; s.async = true;
    s.src = '//cdn.viglink.com/api/vglnk.js';
    var r = d.getElementsByTagName(t)[0]; r.parentNode.insertBefore(s, r);
  }(document, 'script'));
</script>

</body>
</html>
